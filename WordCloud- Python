# -*- coding: utf-8 -*-
"""WordCloud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    ****
"""
!pip install stemming==1.0
!pip install -U nltk
!pip install wordcloud

from stemming.porter2 import stem
import pandas as pd
import csv
import numpy as np 
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud

helen_tweet = pd.DataFrame.from_csv('/content/drive/My Drive/New/helen_text.csv', sep='\t')

#random characters,urls and emojis have already been removed
#stemming text- faster to implement
helen_list = list(helen_tweet['Tweets'])
clean_helen_list = [str(i) for i in helen_list]
documents = [[stem(word) for word in sentence.split(" ")] for sentence in clean_helen_list]

#removing stopwords and non english words
nltk.download("stopwords")
helen_filtered_words = [word for word in documents if word not in stopwords.words('english')]
nltk.download('words')
dict_words = list(set(nltk.corpus.words.words()))

#removing words with less than 4 letters
dict_words = [x for x in dict_words if len(x) > 4]

#converting list to string
dict_str = ', '.join(dict_words)

#creating word-cloud
def generate_wordcloud(text):
  wordcloud = WordCloud(background_color="black",
                        max_words = 400,
                        max_font_size = 40,
                        width=400,
                        height=200,
                        random_state = 42,
                        relative_scaling = 1.0,
                        stopwords = {'Solea','nepenthe','Broadbrim','northward','upper'}).generate(text) #removing words that appeared frequently but don't add much information to word-cloud
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis("off")
  plt.figure(figsize=(100,50), dpi=100)
  plt.show()

generate_wordcloud(dict_str)
